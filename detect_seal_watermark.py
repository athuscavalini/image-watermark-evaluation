#!/usr/bin/env python3

import videoseal
from PIL import Image
import torchvision.transforms as T
from pathlib import Path
import pandas as pd
import os

def detect_watermark(model, img_path, N=5):
    """
    Detecta watermark em uma imagem
    Returns: (detected, confidence, binary_message)
    """
    try:
        img = Image.open(img_path)

        # Converter para RGB se necess√°rio
        if img.mode != 'RGB':
            img = img.convert('RGB')

        # Converter para tensor
        img_tensor = T.ToTensor()(img).unsqueeze(0)

        # Detectar watermark
        # Testar N vezes para obter m√©dia e desvio padr√£o da confian√ßa
        tests = []
        for _ in range(N):
            # Detectar
            detected = model.detect(img_tensor)

            # CORRE√á√ÉO: preds[0,0] n√£o √© confi√°vel!
            # Usar a magnitude m√©dia dos bits da mensagem (preds[0, 1:])
            # Imagens com watermark t√™m bits com valores absolutos MUITO maiores
            message_bits = detected["preds"][0, 1:]  # 256 bits da mensagem
            avg_magnitude = message_bits.abs().mean().item()  # Magnitude m√©dia

            tests.append(avg_magnitude)

        # M√©dia dos resultados
        avg_watermark_magnitude = sum(tests) / len(tests)

        # Threshold baseado na an√°lise:
        # - Imagens com watermark: magnitude m√©dia ~5-10
        # - Imagens sem watermark: magnitude m√©dia ~0.3-0.8
        # Threshold conservador em 2.0
        is_detected = avg_watermark_magnitude > 2.0
        confidence = avg_watermark_magnitude

        return is_detected, confidence

    except Exception as e:
        print(f"Erro ao processar {img_path}: {e}")
        return None, None, None

def evaluate_directory(model, img_dir, label="unknown"):
    """
    Avalia todas as imagens em um diret√≥rio
    """
    img_path = Path(img_dir)
    images = list(img_path.glob('*.jpg')) + list(img_path.glob('*.png'))

    results = []

    for img_file in images:
        detected, confidence = detect_watermark(model, img_file)

        if detected is not None:
            results.append({
                'image': img_file.name,
                'directory': label,
                'detected': detected,
                'confidence': confidence
            })

    return results

def run_evaluation(model):
    """
    Executa avalia√ß√£o completa conforme pipeline
    """
    all_results = []

    print("="*60)
    print("Avalia√ß√£o de Robustez do Watermark")
    print("="*60 + "\n")

    # 1. Baseline negativo (imagens reais sem watermark)
    print("1. Baseline Negativo - Imagens Reais (Flickr8)")
    flickr_dir = Path('flickr8/Images')
    if flickr_dir.exists():
        # Testar apenas uma amostra (mesmas 20 imagens)
        import random
        random.seed(42)
        all_flickr = list(flickr_dir.glob('*.jpg'))
        sample_flickr = random.sample(all_flickr, min(20, len(all_flickr)))

        for img_file in sample_flickr:
            detected, confidence = detect_watermark(model, img_file)
            if detected is not None:
                all_results.append({
                    'image': img_file.name,
                    'directory': 'flickr8_original',
                    'attack': 'none',
                    'detected': detected,
                    'confidence': confidence,
                    'expected': False  # N√£o deve ser detectado
                })
        print(f"   ‚úì Processadas {len(sample_flickr)} imagens\n")
    else:
        print(f"   ‚úó Diret√≥rio {flickr_dir} n√£o encontrado\n")

    # 2. Verdadeiros positivos - Imagens com watermark (sem ataque)
    print("2. Verdadeiros Positivos - Imagens com Watermark")
    seal_dir = Path('seal')
    if seal_dir.exists():
        seal_results = evaluate_directory(model, seal_dir, 'seal_original')
        for res in seal_results:
            res['attack'] = 'none'
            res['expected'] = True  # Deve ser detectado
            all_results.append(res)
        print(f"   ‚úì Processadas {len(seal_results)} imagens\n")
    else:
        print(f"   ‚úó Diret√≥rio {seal_dir} n√£o encontrado\n")

    # 3. Teste de robustez - Imagens com ataques
    print("3. Teste de Robustez - Imagens com Ataques")
    attacks_dir = Path('seal_attacks')
    if attacks_dir.exists():
        attack_types = [d.name for d in attacks_dir.iterdir() if d.is_dir()]

        for attack in sorted(attack_types):
            attack_path = attacks_dir / attack
            attack_results = evaluate_directory(model, attack_path, f'seal_{attack}')

            for res in attack_results:
                res['attack'] = attack
                res['expected'] = True  # Ainda deve ser detectado (teste de robustez)
                all_results.append(res)

            detected_count = sum(1 for r in attack_results if r['detected'])
            total = len(attack_results)
            rate = (detected_count / total * 100) if total > 0 else 0
            print(f"   {attack:20s}: {detected_count}/{total} detectadas ({rate:.1f}%)")

        print()
    else:
        print(f"   ‚úó Diret√≥rio {attacks_dir} n√£o encontrado\n")

    # 4. Teste de robustez - Ataques compostos
    print("4. Teste de Robustez - Ataques Compostos (m√∫ltiplas transforma√ß√µes)")
    combined_attacks_dir = Path('seal_combined_attacks')
    if combined_attacks_dir.exists():
        scenario_types = [d.name for d in combined_attacks_dir.iterdir() if d.is_dir()]

        for scenario in sorted(scenario_types):
            scenario_path = combined_attacks_dir / scenario
            scenario_results = evaluate_directory(model, scenario_path, f'seal_{scenario}')

            for res in scenario_results:
                res['attack'] = f'combined_{scenario}'
                res['expected'] = True
                all_results.append(res)

            detected_count = sum(1 for r in scenario_results if r['detected'])
            total = len(scenario_results)
            rate = (detected_count / total * 100) if total > 0 else 0
            print(f"   {scenario:25s}: {detected_count}/{total} detectadas ({rate:.1f}%)")

        print()
    else:
        print(f"   ‚ö† Diret√≥rio {combined_attacks_dir} n√£o encontrado (execute apply_combined_attacks.py)\n")

    # Criar DataFrame com resultados
    df = pd.DataFrame(all_results)

    # Salvar resultados completos
    output_file = 'watermark_detection_results.csv'
    df.to_csv(output_file, index=False)
    print(f"\n‚úì Resultados salvos em: {output_file}\n")

    # Gerar relat√≥rio resumido
    generate_report(df)

    return df

def generate_report(df):
    """
    Gera relat√≥rio resumido com m√©tricas
    """
    print("="*60)
    print("RELAT√ìRIO DE RESULTADOS")
    print("="*60 + "\n")

    # M√©tricas gerais
    if 'expected' in df.columns:
        # 1. Falsos Positivos (imagens reais detectadas como watermark)
        negatives = df[df['expected'] == False]
        if len(negatives) > 0:
            fp_count = negatives['detected'].sum()
            fp_rate = (fp_count / len(negatives)) * 100
            print(f"1. Taxa de Falso Positivo (imagens reais):")
            print(f"   {fp_count}/{len(negatives)} detectadas incorretamente ({fp_rate:.1f}%)")
            print(f"   {'‚úì √ìtimo' if fp_rate < 5 else '‚ö† Aten√ß√£o'}\n")

        # 2. Verdadeiros Positivos (sem ataque)
        original = df[(df['attack'] == 'none') & (df['expected'] == True)]
        if len(original) > 0:
            tp_count = original['detected'].sum()
            tp_rate = (tp_count / len(original)) * 100
            print(f"2. Taxa de Detec√ß√£o (imagens com watermark, sem ataque):")
            print(f"   {tp_count}/{len(original)} detectadas ({tp_rate:.1f}%)")
            print(f"   {'‚úì √ìtimo' if tp_rate > 95 else '‚ö† Aten√ß√£o'}\n")

    # 3. Robustez por tipo de ataque
    attacked = df[df['attack'] != 'none']
    if len(attacked) > 0:
        # Separar ataques simples e compostos
        simple_attacks = attacked[~attacked['attack'].str.startswith('combined_')]
        combined_attacks = attacked[attacked['attack'].str.startswith('combined_')]

        if len(simple_attacks) > 0:
            print(f"3. Robustez - Ataques Simples:")
            print("-" * 60)

            attack_summary = simple_attacks.groupby('attack').agg({
                'detected': ['sum', 'count', 'mean']
            }).round(3)

            attack_summary.columns = ['Detectadas', 'Total', 'Taxa']
            attack_summary['Taxa %'] = (attack_summary['Taxa'] * 100).round(1)
            attack_summary = attack_summary.sort_values('Taxa', ascending=False)

            print(attack_summary[['Detectadas', 'Total', 'Taxa %']].to_string())
            print()

        if len(combined_attacks) > 0:
            print(f"\n4. Robustez - Ataques Compostos (m√∫ltiplas transforma√ß√µes):")
            print("-" * 60)

            # Remover prefixo 'combined_' para melhor visualiza√ß√£o
            combined_attacks_clean = combined_attacks.copy()
            combined_attacks_clean['attack'] = combined_attacks_clean['attack'].str.replace('combined_', '')

            combined_summary = combined_attacks_clean.groupby('attack').agg({
                'detected': ['sum', 'count', 'mean']
            }).round(3)

            combined_summary.columns = ['Detectadas', 'Total', 'Taxa']
            combined_summary['Taxa %'] = (combined_summary['Taxa'] * 100).round(1)
            combined_summary = combined_summary.sort_values('Taxa', ascending=False)

            print(combined_summary[['Detectadas', 'Total', 'Taxa %']].to_string())
            print()

        # Classificar todos os ataques por severidade
        print(f"\n5. Classifica√ß√£o Geral de Ataques por Impacto:")
        print("-" * 60)

        all_attack_summary = attacked.groupby('attack').agg({
            'detected': ['sum', 'count', 'mean']
        }).round(3)
        all_attack_summary.columns = ['Detectadas', 'Total', 'Taxa']
        all_attack_summary['Taxa %'] = (all_attack_summary['Taxa'] * 100).round(1)
        all_attack_summary = all_attack_summary.sort_values('Taxa', ascending=False)

        for idx, row in all_attack_summary.iterrows():
            rate = row['Taxa %']
            impact = "BAIXO" if rate > 80 else "M√âDIO" if rate > 50 else "ALTO"
            emoji = "‚úì" if rate > 80 else "‚ö†" if rate > 50 else "‚úó"
            attack_name = idx.replace('combined_', 'üîó ')
            print(f"   {emoji} {attack_name:30s}: {rate:5.1f}% - Impacto {impact}")

        # Compara√ß√£o: ataques simples vs compostos
        if len(simple_attacks) > 0 and len(combined_attacks) > 0:
            print(f"\n6. Compara√ß√£o: Ataques Simples vs Compostos:")
            print("-" * 60)
            simple_avg = simple_attacks['detected'].mean() * 100
            combined_avg = combined_attacks['detected'].mean() * 100
            print(f"   Taxa m√©dia - Ataques Simples:   {simple_avg:.1f}%")
            print(f"   Taxa m√©dia - Ataques Compostos: {combined_avg:.1f}%")
            print(f"   Diferen√ßa: {simple_avg - combined_avg:+.1f}% pontos percentuais")
            if combined_avg < simple_avg - 10:
                print(f"   ‚ö† Ataques compostos s√£o significativamente mais efetivos!")
            elif combined_avg < simple_avg:
                print(f"   ‚Üí Ataques compostos reduzem a taxa de detec√ß√£o")
            else:
                print(f"   ‚úì Watermark robusto at√© para ataques compostos")

    print("\n" + "="*60)

if __name__ == '__main__':
    print("Carregando modelo Seal...")

    # Carregar modelo
    model_card_path = Path(os.path.dirname(videoseal.__file__)) / 'cards' / 'videoseal_1.0.yaml'
    model = videoseal.load(model_card_path)

    print("Modelo carregado com sucesso!\n")

    # Executar avalia√ß√£o
    results_df = run_evaluation(model)
